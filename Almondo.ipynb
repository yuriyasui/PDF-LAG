{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOJY+I3INcr+8OE1VbTQv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuriyasui/PDF-RAG/blob/main/Almondo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPENAI_KEY\n",
        "OPENAI_API_KEY = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "PR_N80wlBO_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"On which datasets does GPT-3 struggle?\" # \"What's the differences between zero-shot and one-shot?\"\n",
        "PDF = 'https://arxiv.org/pdf/2005.14165.pdf'"
      ],
      "metadata": {
        "id": "NQ94BceuzGP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWmbPq6DiWAS"
      },
      "outputs": [],
      "source": [
        "# Install necessary module\n",
        "# print(\"Installing ....\")\n",
        "!pip install langchain openai pypdf faiss-gpu tiktoken --quiet\n",
        "# print(\"Success!!!!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Document loading\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# file = 'https://arxiv.org/pdf/2005.14165.pdf'\n",
        "# loader = PyPDFLoader(file)\n",
        "# document = loader.load()\n",
        "\n",
        "# from langchain.document_loaders import PagedPDFSplitter\n",
        "\n",
        "# file = 'https://arxiv.org/pdf/2005.14165.pdf'\n",
        "# document = PagedPDFSplitter(file)\n",
        "# pages = document.load_and_split()\n",
        "\n",
        "# for i in range(10, 15):\n",
        "#   print('Page number:', i)\n",
        "#   print(pages[i].page_content)\n",
        "#   print(' ')"
      ],
      "metadata": {
        "id": "sG1Y-_jx8rLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document loading\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "file = PDF\n",
        "loader = PyPDFLoader(file)\n",
        "document = loader.load()"
      ],
      "metadata": {
        "id": "SKwU2bxI196w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split document into chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "pages = text_splitter.split_documents(document)\n",
        "\n",
        "# for i in range(10, 15):\n",
        "#   print('Page number:', i)\n",
        "#   print(pages[i].page_content)\n",
        "#   print(' ')\n"
      ],
      "metadata": {
        "id": "WyABfZ7qpIuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "db = FAISS.from_documents(pages, embeddings)"
      ],
      "metadata": {
        "id": "tpQxQVIu-uG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever\n",
        "\n",
        "query = Question\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
        "results = retriever.get_relevant_documents(query)\n",
        "\n",
        "for result in results:\n",
        "    print(str(result.metadata[\"page\"]) + \":\", result.page_content)"
      ],
      "metadata": {
        "id": "TRbyJdtcBoqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4499c2e4-4fd4-474e-8ca2-c8b1d0b9e078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: achieves strong performance on many NLP datasets, including translation, question-answering, and\n",
            "cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as\n",
            "unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\n",
            "time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\n",
            "datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\n",
            "we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty\n",
            "distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding\n",
            "and of GPT-3 in general.\n",
            "∗Equal contribution\n",
            "†Johns Hopkins University, OpenAI\n",
            "Author contributions listed at end of paper.arXiv:2005.14165v4  [cs.CL]  22 Jul 2020\n",
            "32: predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although\n",
            "the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to\n",
            "lose coherence over sufﬁciently long passages, contradict themselves, and occasionally contain non-sequitur sentences\n",
            "or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of\n",
            "GPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed\n",
            "informally that GPT-3 seems to have special difﬁculty with “common sense physics”, despite doing well on some\n",
            "datasets (such as PIQA [ BZB+19]) that test this domain. Speciﬁcally GPT-3 has difﬁculty with questions of the type\n",
            "“If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable\n",
            "30: Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation\n",
            "split of our training distribution. Though there is some gap between training and validation performance, the gap grows\n",
            "only minimally with model size and training time, suggesting that most of the gap comes from a difference in difﬁculty\n",
            "rather than overﬁtting.\n",
            "although models did perform moderately better on data that overlapped between training and testing, this did not\n",
            "signiﬁcantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\n",
            "GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of\n",
            "magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential\n",
            "for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B\n",
            "4: relative to ﬁne-tuned models operating in the same closed-book setting.\n",
            "GPT-3 also displays one-shot and few-shot proﬁciency at tasks designed to test rapid adaption or on-the-ﬂy reasoning,\n",
            "which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them\n",
            "deﬁned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human\n",
            "evaluators have difﬁculty distinguishing from human-generated articles.\n",
            "At the same time, we also ﬁnd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This\n",
            "includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE\n",
            "or QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we\n",
            "hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.\n",
            "17: multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread\n",
            "in GPT-3’s performance across these datasets suggestive of varying capability with different answer formats. In general\n",
            "we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each\n",
            "respective dataset.\n",
            "GPT-3 performs best (within 3 points of the human baseline) on CoQA [ RCM19 ] a free-form conversational dataset\n",
            "and performs worst (13 F1 below an ELMo baseline) on QuAC [ CHI+18] a dataset which requires modeling structured\n",
            "dialog acts and answer span selections of teacher-student interactions. On DROP [ DWD+19], a dataset testing discrete\n",
            "reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the ﬁne-tuned\n",
            "BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI bot that responds to a question based on the knowledge provided by me.\"), #AI: need to improve the prompt\n",
        "        (\"human\", \"Using the provided information: {text}, please answer the following question: {question}. Provide answer with some examples and the reasoning\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "64iV4OcSDgEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "OvIvGdV9-Hcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chaining\n",
        "\n",
        "chain = chat_template | llm\n",
        "answer = chain.invoke(\n",
        "    {\n",
        "        \"text\": results,\n",
        "        \"question\": Question,\n",
        "    }\n",
        ")\n",
        "print(answer.content)"
      ],
      "metadata": {
        "id": "gBlF8I0aDXiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3930dbb3-9aa5-4281-a744-b0322c1d628b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-3 struggles on certain datasets despite its strong performance on many NLP tasks. Some examples of datasets where GPT-3 faces challenges include:\n",
            "\n",
            "1. ANLI dataset: GPT-3 struggles with natural language inference tasks on this dataset.\n",
            "2. RACE dataset: GPT-3 faces difficulties with some reading comprehension datasets like RACE.\n",
            "3. QuAC dataset: GPT-3 performs poorly on QuAC, a dataset that requires modeling structured dialog acts and answer span selections of teacher-student interactions.\n",
            "\n",
            "These limitations highlight areas where GPT-3's few-shot learning capabilities are not as effective, emphasizing the need for further research and improvement in these specific domains.\n"
          ]
        }
      ]
    }
  ]
}